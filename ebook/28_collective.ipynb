{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121561ab-2583-41a9-95bb-3dbfc693b2a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MPI Collective Operations\n",
    "\n",
    "Collective operators use the entire communicator to perform:\n",
    "\n",
    "* Synchronization - processes wait until all members of the group have reached the synchronization point.\n",
    "* Data Movement - broadcast, scatter/gather, all to all.\n",
    "* Collective Computation (reductions) - one member of the group collects data from the other members and performs an operation (min, max, add, multiply, etc.) on that data.\n",
    "\n",
    "~\n",
    "\n",
    "<img src=\"https://hpc-tutorials.llnl.gov/mpi/images/collective_comm.gif\" width=512 />\n",
    "\n",
    "An MPI barrier can be through of as a synchronous collective message with no data.\n",
    "\n",
    "> `MPI_barrier(com)`: Synchronization operation. Creates a barrier synchronization in a group. Each task, when reaching the MPI_Barrier call, blocks until all tasks in the group reach the same MPI_Barrier call. Then all tasks are free to proceed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe2bd4-a591-4a74-937f-bc21158a1416",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### All Reduce\n",
    "\n",
    "This section drawn  from https://tech.preferred.jp/en/blog/technologies-behind-distributed-deep-learning-allreduce/.\n",
    "\n",
    "The critical operation in distributed deep-learning has become all-reduce.\n",
    "\n",
    "<img src=\"https://tech.preferred.jp/wp-content/uploads/2018/07/fig_1.png\" width=386 />\n",
    "\n",
    "The blog states --\n",
    "> In synchronized data-parallel distributed deep learning, the major computation steps are:\n",
    "> 1. Compute the gradient of the loss function using a minibatch on each GPU.\n",
    "> 2. Compute the mean of the gradients by inter-GPU communication.\n",
    "> 3. Update the model.\n",
    "\n",
    "This is an early effort (2017) in the space. It goes on to describe the ring algorithm, which has been used for a long time in HPC.  It takes two passes around the ring.\n",
    "  * The first collects one element per node. $O(n^2)$ messages in $n-1$ steps.\n",
    "  * The next distributes data to each node.  Same complexity.\n",
    "  \n",
    "<img src=\"https://tech.preferred.jp/wp-content/uploads/2018/07/fig_2.png\" width=256 />  \n",
    "<img src=\"https://tech.preferred.jp/wp-content/uploads/2018/07/fig_4.png\" width=256 />\n",
    "<img src=\"https://tech.preferred.jp/wp-content/uploads/2018/07/fig_5.png\" width=256 />\n",
    "\n",
    "All-Reduce is actually a compound operation. It consists of two phases:\n",
    "  * scatter-reduce: reduce all elements in a distributed fashion\n",
    "  * all-gather: collect the reduced elements\n",
    "  \n",
    "We see this in the two-phases of the ring algorithm.\n",
    "\n",
    "The ring reduce algorithm is implemented by TensorFlow. It is a simple algorithm that has nice properties.\n",
    "\n",
    "Another [blog](https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/) writes:\n",
    "\n",
    "> In the system we described, each of the N GPUs will send and receive values $N-1$ times for the scatter-reduce, and $N-1$ times for the allgather. Each time, the GPUs will send $K / N$ values, where $K$ is the total number of values in array being summed across the different GPUs. Therefore, the total amount of data transferred to and from every GPU is $2(N−1)KN$ which, crucially, is independent of the number of GPUs.\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee95d1-a7f0-4799-abb1-fcf4317e1399",
   "metadata": {},
   "source": [
    "### All-Reduce Strategies\n",
    "\n",
    "There are a variety of more complex reduction strategies that have latency/data tradeoffs. A [2013 paper](https://arxiv.org/abs/1312.3020) characterizes the tradeoffs.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-hwfc8HGjvbOpUR4.png\" width=386 />\n",
    "\n",
    "We cannot apply a simple work/span analsysis of these algorithms. Because the message sizes and the number of work per node varies.  For example, in the \n",
    "tree all-reduce:\n",
    "  * messages in the first half of the protocol double in size\n",
    "  * messages in the second half of the tree are the entire array\n",
    "Tree distribution does send a minimum amount of data at the expense of poor parallelism.\n",
    "  \n",
    "Round robin all-reduce has asymptoticly optimal bandwidth. The down side is that it sends lots of messages.  It is perfectly parallel. \n",
    "\n",
    "Butterfly networks minimize rounds of communication. They also can be mapped to HPC networks with physical restrictions, specifically map the reduction to the specific network topology.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035781a-c606-44cd-a1aa-9858b1a202d8",
   "metadata": {},
   "source": [
    "### NCCL NVidia Collective Communication Library\n",
    "\n",
    "> The NVIDIA Collective Communications Library (NCCL, pronounced “Nickel”) is a library providing inter-GPU communication primitives that are topology-aware and can be easily integrated into applications.\n",
    "\n",
    ">NCCL implements both collective communication and point-to-point send/receive primitives. It is not a full-blown parallel programming framework; rather, it is a library focused on accelerating inter-GPU communication.\n",
    "\n",
    "Essentially, NCCL is a set of MPI routines for GPUs over NVidia \n",
    "\n",
    "> The AllGather operation gathers N values from k ranks into an output of size k*N, and distributes that result to all ranks. The output is ordered by rank index. The AllGather operation is therefore impacted by a different rank or device mapping.\n",
    "\n",
    "<img src=\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/_images/allgather.png\" width=512 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589c462-c986-4b37-ad9f-137f837a2cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
