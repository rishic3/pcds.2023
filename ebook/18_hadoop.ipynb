{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop! A First Example\n",
    "\n",
    "I've installed Hadoop! map/reduce in `/hadoop-3.2.4` The following shows how to build and run the Wordcount example.\n",
    "\n",
    "```bash\n",
    "/hadoop-3.2.4/bin/hadoop com.sun.tools.javac.Main WordCount.java\n",
    "jar cf wc.jar WordCount*.class\n",
    "/hadoop-3.2.4/bin/hadoop jar wc.jar WordCount ../../data/textbible /tmp/output\n",
    "```\n",
    "\n",
    "**Note**: The environment includes:\n",
    "```\n",
    "export JAVA_HOME=/opt/conda/bin\n",
    "export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar\n",
    "```\n",
    "\n",
    "### What did we learn??\n",
    "\n",
    "* Hadoop programs are written in Java (natively)\n",
    "  * Compiled and packed into a Java archive file.\n",
    "* The Hadoop executable takes a jar archive and submits it to the engine\n",
    "  * You should think about this as a job submission engine, similar to HPC schedulers.\n",
    "* Hadoop takes **HDFS** file paths as input and write **HDFS** files as output\n",
    "  * These are actually local file system paths.....more on this later.\n",
    "  \n",
    "### A Java Map/Reduce Program\n",
    "\n",
    "This example is taken from the [Map/Reduce tutorial](https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html).\n",
    "\n",
    "[WordCount.java](./examples/hadoop_wordcount/WordCount.java)\n",
    "\n",
    "A Map/Reduce program contains class that defines a `map()` and `reduce()` function from the interfaces in:\n",
    "* Java package org.apache.hadoop.io.mapreduce\n",
    "  * Mapper (interface for mapper function)\n",
    "  * Reducer (interface for reducer function)\n",
    "* Paradigm\n",
    "  * Implement the interfaces\n",
    "  * Called by the Hadoop! runtime\n",
    "  \n",
    "  \n",
    "#### Mapper\n",
    "```java\n",
    " public static class TokenizerMapper\n",
    "       extends Mapper<Object, Text, Text, IntWritable>{\n",
    "\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(Object key, Text value, Context context\n",
    "                    ) throws IOException, InterruptedException {\n",
    "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "      while (itr.hasMoreTokens()) {\n",
    "        word.set(itr.nextToken());\n",
    "        context.write(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```\n",
    "\n",
    "* Defines a schema for input and output key/value pairs\n",
    "  * `<Object, Text, Text, IntWritable>`\n",
    "  * takes any input key, excpects `Text` as value\n",
    "  * outputs a `Text` key and `Intwritable`\n",
    "* Map/Reduce has specific types that it uses as input/output\n",
    "  * `Text` is analagous to Java's `String`\n",
    "  * `IntWritable` is analagous to built int `int`\n",
    "  * These types are classes that Hadoop! knows how to serialize, marshall etc.\n",
    "* Context is a handle to the Hadoop! runtime\n",
    "  * `context.write(word, one);` outputs from mapper to shuffle.\n",
    "\n",
    "  \n",
    "#### Reducer\n",
    "\n",
    "```java\n",
    "public static class IntSumReducer\n",
    "       extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "    private IntWritable result = new IntWritable();\n",
    "\n",
    "    public void reduce(Text key, Iterable<IntWritable> values,\n",
    "                       Context context\n",
    "                       ) throws IOException, InterruptedException {\n",
    "      int sum = 0;\n",
    "      for (IntWritable val : values) {\n",
    "        sum += val.get();\n",
    "      }\n",
    "      result.set(sum);\n",
    "      context.write(key, result);\n",
    "    }\n",
    "  }\n",
    "```\n",
    "\n",
    "* Reducer schema must be of orm `<A,B,A,B>`\n",
    "  * DANGER: reduces is not a transformation, so you cannot change the key type\n",
    "  * Doing so will break the system\n",
    "  * Seems like a poor design\n",
    "  * When we compare with Google MR pseudocode there's no reduce key type.\n",
    "\n",
    "\n",
    "#### The Rest. Job Setup\n",
    "\n",
    "```java\n",
    "   public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    Job job = Job.getInstance(conf, \"word count\");\n",
    "    job.setJarByClass(WordCount.class);\n",
    "    job.setMapperClass(TokenizerMapper.class);\n",
    "    job.setCombinerClass(IntSumReducer.class);\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "```\n",
    "\n",
    "* Configure a job: a class with “public static void main(..)” entry point to be run by Hadoop!\n",
    "* Assign, output types (seems redundant)\n",
    "* Assign input and output directories\n",
    "* Configure mapper, reducer, _combiner???_\n",
    "  * We'll talk about combiners later\n",
    "* Create a client to manipulate the running job\n",
    "* LAUNCH! (on whatever Hadoop! cluster is configured)\n",
    "* Wait for completion\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Systems and Execution Modes\n",
    "\n",
    "Hadoop is a set of services (master, scheduler, workers, HDFS) that can be run in different configurations:\n",
    "* Cluster setup i.e. __fully distributed__  = each service is run as a scalable service on mutliple nodes.\n",
    "    * This is the deployment scenario for big data\n",
    "* Single-node setup, i.e. __pseudo distributed__ = configures all cluster services as Java processes in a single computer\n",
    "    * runs exactly the same as cluster, but with one node.\n",
    "    * node must be installed and configured to run this way\n",
    "* Local (__standalone__) runs all services in a single java process\n",
    "  * good for development and debugging only (not scalable)\n",
    "  * this is what we did\n",
    "  * requires no configuration, just a Java install.\n",
    "  \n",
    "Most Hadoop! jobs run on the cloud as a service. __Amazon EMR__ service = Elastic Map reduce.\n",
    "* Submit a jar file to launcher and configure data in S3\n",
    "* EMR builds a cluster, runs your job, and puts output in S3\n",
    "* Cluster computing at arbitrary scalability on demand.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### And now for an example\n",
    "\n",
    "[Friends of Friends Hadoop!](examples/18_ex_FoF.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map/Reduce Streaming\n",
    "\n",
    "The Hadoop! Map/Reduce implementation is an engine. You can give it arbitrary executable processes that it will run at scale.  As opposed to .jar files.  In the `wordcount_streaming` subdirectory.\n",
    "\n",
    "```bash\n",
    "/hadoop-3.2.4/bin/mapred streaming -input ../../../data/textbible -output /tmp/output4 -mapper mapper.py -reducer reducer.py\n",
    "```\n",
    "\n",
    "It takes two scripts as arguments.\n",
    "\n",
    "### Shell equivalence\n",
    "\n",
    "For the two shell scripts `mapper.py` and `reducer.py` Hadoop! streaming execution is semantically equivalent to \n",
    "```bash\n",
    "cat ../../data/textbible/* | ./mapper.py | sort | ./reducer.py\n",
    "```\n",
    "But, both are different than the java WordCount implementation..........because?\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".  Spoiler alert\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "* Streaming mode in Hadoop! gives a different sorting guarantee\n",
    "* Why?\n",
    "  * There is no schema\n",
    "  * So, it sorts the whole output of mapper.py as a key\n",
    "* This is more restrictive than the default sort\n",
    "  * And, thus, less efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
