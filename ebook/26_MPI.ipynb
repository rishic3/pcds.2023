{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI: Message Passing Interface\n",
    "\n",
    "MPI is the programming interface to high-performance computing (HPC), i.e. supercomputers.\n",
    "\n",
    "* Message passing parallelism\n",
    "* Cluster computing (no shared memory)\n",
    "* Process (not thread oriented)\n",
    "* Parallelism model\n",
    "  * SPMD: by definition\n",
    "* MPI environment\n",
    "  * Application programming interface\n",
    "  * Implemented in libraries\n",
    "  * Support for C/C++ and Fortran\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a reasonable tutorial [https://hpc-tutorials.llnl.gov/mpi/abstract/](https://hpc-tutorials.llnl.gov/mpi/abstract/) as part of a High-Performance computing tutorial series. \n",
    "\n",
    "MPI routines that are most useful for new MPI programmers include:\n",
    "\n",
    "* MPI Environment Management \n",
    "* Point-to-Point Communications\n",
    "* Collective Communications\n",
    "\n",
    "Does not cover advanced topics such as \n",
    "* Derived Data Types\n",
    "* Group and Communicator Management Routines\n",
    "* Virtual Topologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why teach MPI?\n",
    "\n",
    "The communication paradigms, particularly collective communication patterns, are widely used in distributed AI training.\n",
    "\n",
    "MPI despite seeming really old and low-level is highly-optimized and co-deployed with networking stacks and hardware. [This figure](https://www.osti.gov/servlets/purl/1576171) gives some sense of the process. It's from a paper about how to use RDMA to accelerate MPI messaging.\n",
    "\n",
    "<img src=images/mpinetwork.png width=512 />\n",
    "\n",
    "As AI moves to distributed AI, we are seeing a lot of reuse of MPI implementations to do the collective operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPMD: Single program multiple data\n",
    "\n",
    "From wikipedia “Tasks are split up and run simultaneously on multiple processors with different input in order to obtain results faster. SPMD is the most common style of parallel programming.”\n",
    "  * Asynchronous execution of the same program\n",
    "  \n",
    "<img src=\"https://www.sharcnet.ca/help/images/8/8a/SPMD_model.png\" width=512 title=\"SPMD\" />\n",
    "\n",
    "_SPMD_ is confusing, because it seems like it should be part of Flynn's taxonomy. It is not. _SPMD_ is a software programming model. SIMD is an architectural classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first MPI program\n",
    "\n",
    "* Configure the MPI environment\n",
    "* Discover yourself\n",
    "* Take some differentiated activity\n",
    "\n",
    "all demos in `./mpi` Start with `mpimsg.c`\n",
    "\n",
    "* Idioms\n",
    "  * SPMD: all processes run the same program\n",
    "    * MPI_Rank: tell yourself apart from other and customize the local processes behaviours\n",
    "    * Find neighbors, select data region, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI Vision circa 1996 (Poster at Supercomputing)\n",
    "\n",
    "<img src=\"https://www.netlib.org/mpi/mpi.gif\" width=512 />\n",
    "\n",
    "The goals of the MPI process was to normalize message passing, which was previously spread across many different incompatible libraries that were often machine dependent:\n",
    "  * portable (code reuse across different hardware, software)\n",
    "  * multiple vendors\n",
    "  * extensible (value added libraries/tools/applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MPI Toolchain\n",
    "\n",
    "Build and launch scripts that wrap a compiler.\n",
    "    \n",
    "<img src=\"./images/mpiscip.png\" width=400 /> \n",
    "\n",
    "* To compile an MPI program, you call the associated wrapper.\n",
    "* To run an MPI program:\n",
    "  * **debug** `mpirun` to launch MPI job on the local machine/cluster\n",
    "  * **deploy** launch through scheduler on HPC clusters (do not run on the login node)\n",
    "\n",
    "    \n",
    "```\n",
    "mpicc mpimsg.c -o mpimsg\n",
    "mpirun mpimsg\n",
    "mpirun -np 16 --oversubscribe mpimsg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPC Scheduler\n",
    "\n",
    "Schedule many parallel jobs onto a supercomputer based on size, resources needed, priority.\n",
    "* Maui/Torque\n",
    "* SLURM\n",
    "* OGE\n",
    "\n",
    "Each with their own submission scripts. Not mpirun.\n",
    "    \n",
    "HPC systems have login nodes that you `ssh` into.  **Do not call `mpirun` on login nodes**\n",
    "  * this tries to run a parallel job on the login node.\n",
    " \n",
    "<img src=\"https://engaging-web.mit.edu/eofe-wiki/img/Slurm_Diagram.png\" width=512 />  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MPI Runtime\n",
    "\n",
    "MPI programs are just C/Fortran that include message passing directives.\n",
    "One designs an SPMD program that will collaborate to solve a problem that includes:\n",
    "  * Calls to the MPI library\n",
    "  * Interactions with the MPI runtime\n",
    "  \n",
    "Some calls query or manipulate the runtime:  \n",
    "* Initialize the environment\n",
    "  * `MPI_Init ( &argc, &argv )`\n",
    "* Acquire information for process to differentiate process behavior in SMPD\n",
    "  * `MPI_Comm_size ( MPI_COMM_WORLD, &num_procs )`\n",
    "  * `MPI_Comm_rank ( MPI_COMM_WORLD, &ID )`\n",
    "* And cleanup\n",
    "  * `MPI_Finalize()`\n",
    "\n",
    "### MPI Communicators and Groups\n",
    "\n",
    "The MPI runtime has knowledge of the configuration of the cluster. The nodes of the cluster are connected by the global communicator `MPI_COMM_WORLD`. This specifies the number of nodes `MPI_Comm_size`.\n",
    "\n",
    "It is possible to make application/task specific scopes with narrower communicators and groups. For example, you may break the global cluster into nodes with and without GPUs. \n",
    "\n",
    "<img src=\"https://cvw.cac.cornell.edu/mpiadvtopics/communicators-groups/communicators.gif?v=pAsH8Kcxy00ZJiis72tMMunpeqgvBx0cw65cxeq95hw\" width=368 />\n",
    "\n",
    "Most MPI programs and all our examples will use only the global scope.\n",
    "  \n",
    "## MPI Design Ethos\n",
    "* MPI is just messaging.\n",
    "    * And synchronization constructs, which are built on messaging\n",
    "    * And library calls for discovery and configuration\n",
    "* Computation is done in C/C++/Fortran SPMD program\n",
    "* MPI is sometimes called the “assembly language” of supercomputing\n",
    "    * Simple primitives\n",
    "    * Build your own communication protocols, application topologies, parallel execution\n",
    "    * The opposite end of the design space from Dask, Spark in which you write simple declarative programs that are automatically parallelized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
