{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PL6LzCfNfy8"
   },
   "source": [
    "## k-means in Spark\n",
    "\n",
    "We will implement k-means for k=4 with points in 2-dimensions only.  I have provided comments that will provide guidance as to the implementation and left as much skeleton code as possible.  Your implementation should use the Spark RDD interface and keep data in Spark RDDs whenever possible.  If you are writing a for loop, you are doing it wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IT5L0TWVNfy8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"kmeans2d\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fP_rjQZRNfy8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k-means helper functions\n",
    "\n",
    "# assign each point to a cluster based on which centroid is closest\n",
    "# centroids should be a np.array of shape (4,2), dtype=float32\n",
    "def assign_class(point, centroids):\n",
    "\n",
    "    mindist = np.finfo(np.float64).max\n",
    "    for j in range(len(centroids)):\n",
    "        distance = np.linalg.norm(point-centroids[j])\n",
    "        if distance < mindist:\n",
    "            mindist = distance\n",
    "            assignclass = j\n",
    "    return assignclass\n",
    "\n",
    "# plot the data distribution.\n",
    "#\n",
    "# pstriples should be an RDD of type k,v = (int, [float32, float32])\n",
    "# centroids is again np.array of shape (4,2), dtype=float32\n",
    "def plot_clusters(ptstriples, centroids):\n",
    "\n",
    "    # extract the points in each cluster\n",
    "    lcluster0 = ptstriples.filter(lambda x: x[0] == 0).map(lambda x: x[1])\n",
    "    lcluster1 = ptstriples.filter(lambda x: x[0] == 1).map(lambda x: x[1])\n",
    "    lcluster2 = ptstriples.filter(lambda x: x[0] == 2).map(lambda x: x[1])\n",
    "    lcluster3 = ptstriples.filter(lambda x: x[0] == 3).map(lambda x: x[1])\n",
    "\n",
    "    # convert data to np.arrays\n",
    "    cluster0 = np.array(lcluster0.collect())\n",
    "    cluster1 = np.array(lcluster1.collect())\n",
    "    cluster2 = np.array(lcluster2.collect())\n",
    "    cluster3 = np.array(lcluster3.collect())\n",
    "\n",
    "    # plot the cluster data differentiated by color\n",
    "    plt.plot(cluster0[:,0], cluster0[:,1], 'b.', markersize=2)\n",
    "    plt.plot(cluster1[:,0], cluster1[:,1], 'r.', markersize=2)\n",
    "    plt.plot(cluster2[:,0], cluster2[:,1], 'm.', markersize=2)\n",
    "    plt.plot(cluster3[:,0], cluster3[:,1], 'c.', markersize=2)\n",
    "\n",
    "    # overlay the centroids\n",
    "    plt.plot(centroids[:,0], centroids[:,1], 'ko', markersize=5)\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "# plot the intial data before there are labels\n",
    "#\n",
    "# centroids is again np.array of shape (4,2), dtype=float32\n",
    "# points is an RDD\n",
    "def showpoints(points, centroids):\n",
    "    points = np.array(points.collect())\n",
    "    plt.plot(points[:,0], points[:,1], 'b.', markersize=1)\n",
    "    plt.plot(centroids[:,0], centroids[:,1], 'ro', markersize=10)\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdRP06pNR7ik"
   },
   "source": [
    "### Generate Data\n",
    "\n",
    "Create a k-means data set in this spark context.  The default is to create 2000 points, 500 each from 4 distributions. You can change then classcount to create small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqCNv1B2SD0U",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# generate classcount points and permute for each spark partition\n",
    "def gen2000 (i):\n",
    "\n",
    "    # 2 data points in each class for a small dataset\n",
    "    # classcount = 2\n",
    "\n",
    "    # 500 data points in each class for a large dataset\n",
    "    classcount = 500\n",
    "\n",
    "    cov = [[1, 0], [0, 1]]  # diagonal covariance\n",
    "\n",
    "    points1 = np.random.multivariate_normal([2,2], cov, classcount)\n",
    "    points2 = np.random.multivariate_normal([2,-2], cov, classcount)\n",
    "    points3 = np.random.multivariate_normal([-2,2], cov, classcount)\n",
    "    points4 = np.random.multivariate_normal([-2,-2], cov, classcount)\n",
    "\n",
    "    # put all points together and permute\n",
    "    pointsall = np.concatenate((points1, points2, points3, points4), axis=0)\n",
    "    pointsall = np.random.permutation(pointsall)\n",
    "\n",
    "    return pointsall\n",
    "\n",
    "# number of partitions in dataset\n",
    "slices = 4\n",
    "\n",
    "# make points and materialize to an RDD. Then collect.\n",
    "# This prevents the from being randomly regenerated each iteration.\n",
    "# This is an array, not an RDD, because we collect.\n",
    "pointsar = sc.parallelize(range(slices), numSlices=slices).flatMap(gen2000).collect()\n",
    "\n",
    "# get the same points as an RDD everytime\n",
    "points = sc.parallelize(pointsar)\n",
    "\n",
    "# optionally persist the points to cache for reuse.\n",
    "#points.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "a5aoRIEPNfy8",
    "outputId": "ec6fba06-f2b1-4721-9bb8-78dc795ee3a2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take a sample of k points as seeds (comment out the DEBUG line)\n",
    "## TODO\n",
    "\n",
    "# (DEBUG) or use these as an example when debugging\n",
    "centroids = np.array([[2.0,2.0],[2.0,-2.0],[-2.0,2.0],[-2.0,-2.0]])\n",
    "\n",
    "# keep a copy for rerunning\n",
    "originalCentroids = centroids\n",
    "\n",
    "showpoints(points, centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDf0MSCwNfy9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assign each point to a class using the assign_class function\n",
    "# produces an RDD with type (int) with length equal to number of points\n",
    "### TODO\n",
    "# clusters = ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJt29lfvNfy9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build an RDD of type (int, [float, float]) that specifies the cluser and then the point coordinates\n",
    "# this can be done efficiently with with `zip()` function\n",
    "### TODO\n",
    "# ptstriples = ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyQVL-z-Nfy9"
   },
   "source": [
    "Some hints for the next cell\n",
    "\n",
    "   1. use `groupByKey()` to collect data by cluster\n",
    "   2. at the end you are going to have to use\n",
    "         the function `np.mean(array, axis=0`) on a iterator. Keep the data in spark RDDs until the last step.\n",
    "   3. it can be hard to materialize RDDs into arrays you need to either `np.array(RDD)` or `np.array(list(RDDiterable))`\n",
    "   4. I wrote a helper function, rather than using a lambda to help take the mean because it was more readable.\n",
    "   5. be careful with the ordering of your centroids. RDDs are not necessarily sorted by key.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqP4Q9AeNfy9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update the centroids to be the mean of each cluster of points\n",
    "### TODO\n",
    "\n",
    "# ...\n",
    "\n",
    "# centroids = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "LdCxwLS1Nfy9",
    "outputId": "07cea535-b7d9-4d62-b8b3-283c641a69c5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# look at the output of your intial clustering\n",
    "plot_clusters(ptstriples, centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7lL3k_RNfy9"
   },
   "source": [
    "k-means is an iterative algorithm.  You will see that the centroids progessively converge to the true means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4B_jUU4Nfy9"
   },
   "outputs": [],
   "source": [
    "#%%timeit -n 1\n",
    "\n",
    "iterations = 10\n",
    "centroids = originalCentroids\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    ### TODO\n",
    "    # run the whole process over and over\n",
    "    # clusters = ...\n",
    "    # ptstriples = ...\n",
    "    # ...\n",
    "    # centroids = ...\n",
    "\n",
    "    # optionally plot the output (could be slow)\n",
    "    # plot_clusters(ptstriples, centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLf49W98GKkS"
   },
   "source": [
    "### Alternate Implementation\n",
    "\n",
    "Our first implementation used a `groupBy` to collect data by cluster. This has the disadvantage that it shuffles data and collects data by partition. We will do another implementation that moves no data outside of partitions.  This will use the `filter` pattern that is implemented in the `plot_clusters` function.\n",
    "\n",
    "Your program should filter all data within a partition and then aggregate data within each partition. I have given you the `part_sums` helper to aggregate within each partition and the `sums_2_means` helper function to convert the sums into means.\n",
    "\n",
    "Leave all the `persist()` operations and also all the `%%timeit` directives commented out at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "I2BNTJkTGLfp",
    "outputId": "67881acb-af95-42d7-da17-1c75976b8e38"
   },
   "outputs": [],
   "source": [
    "# helper function that returns the sum of the points in an RDD.\n",
    "# HINT: you do want to call this once per partition\n",
    "def part_sums(clusterrdd):\n",
    "    ar = np.array(list(clusterrdd))\n",
    "    return (np.sum(ar, axis=0), ar.shape[0])\n",
    "\n",
    "# helper function to aggregate the sums and counts into means\n",
    "def sums_2_means(sumslist):\n",
    "    sums = sumslist[::2]\n",
    "    counts = sumslist[1::2]\n",
    "    return np.sum(sums, axis=0)/np.sum(counts)\n",
    "\n",
    "# Create an empty array for updated centroids\n",
    "ucentroids = [ None for i in range(4)]\n",
    "\n",
    "# use the orginal centroids as input\n",
    "centroids = originalCentroids\n",
    "\n",
    "###TODO\n",
    "\n",
    "# create clusters and ptstriples as previously\n",
    "clusters = ...\n",
    "ptstriples = ...\n",
    "\n",
    "# For each of the four clusters (repeat this code for all four clusters)\n",
    "\n",
    "# filter for only the points in this cluster\n",
    "lcluster0 = ...\n",
    "\n",
    "# derive means in each cluster (or do it another way)\n",
    "cluster0means = ...\n",
    "\n",
    "# aggregate means from each partition and update centroids\n",
    "ucentroids[0] = ...\n",
    "\n",
    "lcluster1 = ...\n",
    "cluster1means = ...\n",
    "ucentroids[1] = ...\n",
    "\n",
    "lcluster2 = ...\n",
    "cluster2means = ...\n",
    "ucentroids[2] = ...\n",
    "\n",
    "lcluster3 = ...\n",
    "cluster3means = ...\n",
    "ucentroids[3] = ...\n",
    "\n",
    "# optionally plot the clusters (may be slow)\n",
    "plot_clusters(ptstriples, np.array(ucentroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azv_Tje8uTaN",
    "outputId": "6fabbeb4-ccb8-4afd-81ae-da92a8b8fc62"
   },
   "outputs": [],
   "source": [
    "#%%timeit -n 1\n",
    "\n",
    "ucentroids = [ None for i in range(4)]\n",
    "centroids = originalCentroids\n",
    "\n",
    "iterations = 10\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    ### TODO\n",
    "    # create clusters and ptstriples as previously\n",
    "    # clusters = ...\n",
    "    # ptstriples = ...\n",
    "\n",
    "    # optionally persist the triples for cache reuse\n",
    "    #ptstriples.persist()\n",
    "\n",
    "    # run the whole process repeatedly\n",
    "    # lcluster0 = ...\n",
    "    # ...\n",
    "    # ...\n",
    "\n",
    "    # apply updated centroids for next iteration\n",
    "    centroids = np.array(ucentroids)\n",
    "\n",
    "    # optionally plot the clusters (may be slow)\n",
    "    # plot_clusters(ptstriples, np.array(ucentroids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm9yUaO4Nfy9"
   },
   "source": [
    "### Stop the context\n",
    "\n",
    "If you crash, you will often need to close Spark explicitly to reset the system.  Just run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhmcRUZTNfy9"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAP2k_ov4Dms"
   },
   "source": [
    "## Questions\n",
    "\n",
    "We now turn to an evaluation of the relative performance of our two implementations and a study of the benefit of caching.  Performance results are consistent across my 8-core laptop (MacOSX), 12-core laptop (Windows), and an 8-core droplet on Digital Ocean. Your results may vary, but you should be able to explain.\n",
    "\n",
    "#### Question 1\n",
    "\n",
    "Comment out all `plot_clusters` call and uncomment the `%%timeit` decorators. Run the notebook and get the timing information.\n",
    "\n",
    "* How long did each implementation take to run?\n",
    "    * ###TODO\n",
    "\n",
    "* The `groupBy` implementation is faster than the `filter` implementation. Why?\n",
    "    * ###TODO\n",
    "\n",
    "On a distributed-memory cluster, the `filter` implementation will always be faster.\n",
    "\n",
    "* Why would the `filter` implementation run faster on distributed memory?\n",
    "    * ###TODO\n",
    "\n",
    "#### Question 2\n",
    "\n",
    "There are two commented out `persist()` calls in this notebook: one for `points` and one for `ptstriples` in the `filter` implementation.  Run four versions of this code and give performance results (timings from `%%timeit`) for each of the following:\n",
    "\n",
    "* persist neither `points` nor `ptstriples`\n",
    "    * ###TODO\n",
    "* persist `points` but not `ptstriples`\n",
    "    * ###TODO\n",
    "* persist `ptstriples` but not `points`\n",
    "    * ###TODO\n",
    "* persist both `points` and `ptstriples`\n",
    "    * ###TODO\n",
    "    \n",
    "#### Question 3\n",
    "\n",
    "* Caching `ptstriples` in the `filter` implementation makes it faster. Explain why.\n",
    "  * ###TODO\n",
    "  \n",
    "* Why is it more effective to cache `ptstriples` than `points`?\n",
    "  * ###TODO\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
