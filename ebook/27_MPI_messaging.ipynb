{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da51a650-c7f3-403d-8b44-aac8978d95d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MPI Point to Point Messaging\n",
    "\n",
    "Point-to-point communication is the most basic operation in an MPI program. Send a buffer of data from one node to another.\n",
    "\n",
    "| Operation | Syntax |\n",
    "| :--- | ---: |\n",
    "| Blocking send: | `MPI_Send(buffer,count,type,dest,tag,comm)` |\n",
    "| Non-blocking send: | `MPI_Isend(buffer,count,type,dest,tag,comm,request)` |\n",
    "| Blocking receive: | `MPI_Recv(buffer,count,type,source,tag,comm,status)` |\n",
    "| Non-blocking receive: | `MPI_Irecv(buffer,count,type,source,tag,comm,request)` |\n",
    "\n",
    "`buffer` is a program (application) address space that references the data that is to be sent or received. In most cases, this is simply the variable name that is be sent/received. \n",
    "\n",
    "`count` indicates the number of data elements of a particular type to be sent.\n",
    "\n",
    "`type` for reasons of portability, MPI predefines its elementary data types.\n",
    "\n",
    "The `Send/Recv` operations a __synchronous__.\n",
    "  * The sender does not return until the buffer has been transferred.\n",
    "  * The receiver does not return until the message is received.\n",
    "  \n",
    "Isend/Irecv operations are __asychronous__. They return immediately.\n",
    "  * The sender must keep the buffer intact (cannot reuse or destroy) until the send happens at a later time.\n",
    "  * The receceiver gives a buffer that will be filled at a later time. It must check the status for completion.\n",
    "  * If the sender/receiver overwrites or deallocates the buffer, unknown wrong things will happen.\n",
    "   \n",
    "Asynchronous messaging is more challenging in MPI than Ray. In Ray, the system took possesion of the buffers into the key/value store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0995a6-7bf7-48e4-a832-afbb3a668b20",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Synchronous Sends and Deadlock\n",
    "\n",
    "With blocking send and receive, we know how to build a deadlock.  Let's try it in [examples/mpi/nodeadlock.c](examples/mpi/nodeadlock.c). We'll do this with four nodes where the processes are sends and the resources on which we are waiting are receives.  Recall our deadlock picture with 3 nodes.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Wait-for_graph_example.svg/2880px-Wait-for_graph_example.svg.png\" width=256 />\n",
    "\n",
    "OK, that didn't work, why not?\n",
    "\n",
    "The MPI messaging systems is running in something called standard mode. The gotcha here is:\n",
    "\n",
    "> MPI_Send will not return until you can use the send buffer. It may or may not block (it is allowed to buffer, either on the sender or receiver side, or to wait for the matching receive).\n",
    "\n",
    "This creates a real hazard. __Why?__ _discussion in HTML comments_\n",
    "\n",
    "\n",
    "Buffering may work sometime and have the effect of turning potentially synchronous calls into asynchronous calls.  This adds queues to avoid deadlocks as we did in the ray examples.  The bad thing that happens:\n",
    "* Build an MPI program and test it. It runs fine.\n",
    "* Deploy it to a new environment. It deadlocks.\n",
    "\n",
    "\n",
    "Buffering may be used when available and not other times. It depends on available memory and configuration of the cluster. Conditions in which buffering might stop working.\n",
    "* Many sender or receivers\n",
    "* Increased message size\n",
    "\n",
    "Essentially, when your program scales up it breaks.\n",
    "\n",
    "OK, let's make the deadlock happen [examples/mpi/deadlock.c](examples/mpi/deadlock.c)\n",
    "\n",
    "The secret here is:\n",
    "\n",
    "> MPI_Ssend: Send a message and block until the application buffer in the sending task is free for reuse and the destination process has started to receive the message.\n",
    "\n",
    "Interesting, it's not fully synchronous. The receiver has not gotten the entire message yet. But, if the receiver is single threaded, it's thread safe. What could happen if the receiver is itself a parallel program?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2940c-fa58-4f07-914e-c715b82e3d84",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Eliminating Deadlock\n",
    "\n",
    "Paired/sends and receives. [examples/mpi/passitforward.c](examples/mpi/passitforward.c)\n",
    "\n",
    "<img src=./images/pairedsr.png width=512>\n",
    "\n",
    "This is the two-phase, deadlock-free protocol we talked about in Ray.\n",
    "\n",
    "The canonical form of deadlock avoidance is the [Banker's Algorithm](https://en.wikipedia.org/wiki/Banker%27s_algorithm) that performs admission based on the current allocated set of resources. It requires knowledge of all system resources and the max resources needed by each process.\n",
    "\n",
    "### Best Practice\n",
    "\n",
    "Develop a deadlock-free MPI program by:\n",
    "* Implementing code with `MPI_Ssend` to reveal deadlocks in the messaging protocol.\n",
    "* Deploy code with `MPI_Send` so that the system can optimize performance at runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b567488-7d5a-4d1e-bcc1-55bf6960aeff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
