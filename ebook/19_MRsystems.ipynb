{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map/Reduce Semantics and Systems\n",
    "\n",
    "### Types and transformations\n",
    "* Map is a transformation\n",
    "  * Input domain to output domain\n",
    "* Reduce is a collection\n",
    "  * No domain change\n",
    "  \n",
    "$$\n",
    "map (k1, v1) \\rightarrow list(k2,v2) \\\\\n",
    "reduce (k2, list(v2)) \\rightarrow list(v2)\n",
    "$$\n",
    "\n",
    "* Google C++ implementation is based all on strings\n",
    "  * User code must convert to structured types\n",
    "* Hadoop! has type wrappers\n",
    "\n",
    "### Parallelism in Map/Reduce\n",
    "\n",
    "* How much potential paralleism in mappers? in reducers?\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "......spoiler alert.......\n",
    "  \n",
    ".  \n",
    "\n",
    ".\n",
    "* Mappers: up to a parallel process for each input (typically a file)\n",
    "* Reducers: up to a parallel process for each key\n",
    "* So for the WordCount example\n",
    "  * two files = two mappers\n",
    "  * 5 different words = five reducers\n",
    "  * but this is scalable with input\n",
    "\n",
    "\n",
    "Differentiating betweens mapper/reducer and map/reduce processes\n",
    "* A cluster will typically configure number of available phyical processes\n",
    "  * this number is typically much smaller than potential parallelism\n",
    "  * we refer to `mappers` as potential parallelism and `map processes` as the number of phyical processes running map functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MR Semantics\n",
    "\n",
    "Let's start with some definitions:\n",
    "\n",
    "#### Shuffle\n",
    "\n",
    "This is the routing process of mapper outputs to reducer inputs.\n",
    "\n",
    "<img src=\"./images/hadoopsem.png\" width=512 />\n",
    "\n",
    "#### Partition\n",
    "\n",
    "* Partition is the output file of a reducer _process_ (not a single reducer).\n",
    "  * Contains many reducer keys\n",
    " \n",
    "#### Combiner\n",
    "\n",
    "This image is linked from https://data-flair.training/blogs/hadoop-combiner-tutorial/.  Please refer to their page.\n",
    "\n",
    "<img src=\"images/combiner.jpeg\" width=512 />\n",
    "\n",
    "Combiner is a function that runs on the outputs of the mapper before the shuffle.\n",
    "\n",
    "* Combiner executes on the mappers $ \\langle key,value \\rangle$ output while in memory at the mapper\n",
    "  * It is possible to write unique combiner and reduce classes\n",
    "  * It is common to use the reducer as a combiner\n",
    "  * Combiner must have algebraic propertics, i.e. `reduce(combine(A),combine(B)) == reduce(A,B)` \n",
    "  * Traditional reduce operators (aggregates and extrema) work in combiners\n",
    "* Combiner in WordCount:\n",
    "  * compute sum output by mapper for each key and send a single aggregated value to reducers\n",
    "* Combiner for Maximum: \n",
    "  * compute maximum value for each key.  \n",
    "  * Reducer computes a maximum of maxima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map/Reduce Runtimes\n",
    "\n",
    "From the Google paper https://www.usenix.org/legacy/events/osdi04/tech/dean.html\n",
    "\n",
    "<img src=\"./images/mr.png\" width=512 />\n",
    "\n",
    "* Automatically partition input data\n",
    "  * 16-64 MB chunks for Google\n",
    "* Create M map tasks: one for each chunk\n",
    "  * Assign available workers (up to M) to tasks\n",
    "* Write intermediate pairs to local (to worker) disk\n",
    "* R reduce tasks (defined by user) read and process intermediate results\n",
    "* Output is up to R files available on shared file system\n",
    "* Master tracks state\n",
    "  * Asssignment of M map tasks and R reduce tasks to workers\n",
    "  * State and liveliness of the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systems Issues\n",
    "\n",
    "The map/reduce runtime must deal with:\n",
    "* Master failure\n",
    "  * Checkpoint/restart, classic distributed systems/replication problem\n",
    "* Failed worker\n",
    "  * Heartbeat liveness detection, restart\n",
    "* Slow worker\n",
    "  * Backup tasks\n",
    "* Locality of processing to data\n",
    "  * Big deal, they donâ€™t really solve\n",
    "  * But, much subsequent research does\n",
    "* Task granularity\n",
    "  * Metadata size and protocol scaling (not inherent parallelism) limit the size of M and R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
